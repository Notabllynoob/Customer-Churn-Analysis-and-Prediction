{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC2BFMOKN1M7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR56trFcPcC1"
   },
   "source": [
    "# Churn Prediction\n",
    "\n",
    "**\"Churn rate\"** , or simply **\"churn\"**, represents the rate of customer base evasion. In services such as Spotify or Netflix, it would represent the rate of subscription cancellations.\n",
    "\n",
    "\n",
    "\n",
    "It is of utmost importance for management, and its analysis over time can show that there is a problem that needs to be addressed.\n",
    "\n",
    "\"Churn\" can also be used to identify potential cancellations in advance and promote targeted actions to try to retain such customers. This metric should receive attention because the Cost of Customer Acquisition (CAC) is typically higher than the cost to retain them. In other words, a high value for the churn rate is undesirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HVmpIxQWT4Y"
   },
   "source": [
    "## Data Acquisition\n",
    "\n",
    "The data used in this project was originally made available at [IBM Developer](https://developer.ibm.com/technologies/data-science/patterns/predict-customer-churn-using-watson-studio-and-jupyter-notebooks/#), and deals with a typical problem of a telecommunications company. The complete dataset can be found [here](https://raw.githubusercontent.com/carlosfab/dsnp2/master/datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv).\n",
    "\n",
    "Although there is no explicit information available, the column names allow for an understanding of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9ZSROzUfQAj",
    "outputId": "ee24bcc8-33e5-427e-f859-227a87bb8fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in g:\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in g:\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in g:\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in g:\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in g:\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in g:\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in g:\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# installing the necessary packages\n",
    "!pip install scikit-learn -q\n",
    "!pip install --upgrade imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "oQ4bNy7udtEE",
    "outputId": "f73029a7-a4a4-4b37-979b-7ecf9d02f270"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SGDClassifier\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# importing the necessary packages\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# importing the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# importing the data\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/carlosfab/dsnp2/master/datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# checking the first entries\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF6VMZ1SwwLq"
   },
   "source": [
    "Visualizing the first 5 entries allows us to understand structure of the dataset. Even though there is no extra material with more details about how the data was collected and organized, it is stills possible to understand and work with this data.\n",
    "\n",
    "As shown below, the dataset is composed of 7043 entries and 21 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvBoPMD7QVMY",
    "outputId": "d8403e4b-71a1-4f24-d02f-22cf445b66c4"
   },
   "outputs": [],
   "source": [
    "print(\"Rows:\\t\\t{}\".format(df.shape[0]))\n",
    "print(\"Columns:\\t{}\".format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09YZmEtrn9sL"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "To start off this project, let's check the integrity and usability of the dataset.\n",
    "\n",
    "To do this, I will print out the amount of missing data, the column names, the variable types per column, and the unique values per column. This last one, specifically, will help in identifying potentially categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebSgr1nGoFvW",
    "outputId": "a0b158d7-9f2e-44b8-ae95-b5ab6fa92875"
   },
   "outputs": [],
   "source": [
    "print(\"Missing Data (total):\\t{}\\n\".format(df.isnull().sum().values.sum()))\n",
    "print(df.isnull().sum() / df.shape[0])\n",
    "print(\"\\nFeatures:\\n{}\\n\".format(df.columns.values))\n",
    "print(\"Unique Values (by column):\\n{}\\n\".format(df.nunique()))\n",
    "print(\"Variable Type (by column)\\n{}\".format(df.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBmv0Haypvsk"
   },
   "source": [
    "Let's go over the highlights:\n",
    "\n",
    "\n",
    "* `Churn` is the target variable, indicating the situation of the client regarding this metric.\n",
    "* `TotalCharges` should be a `float`, but is currently `string`.\n",
    "    * As of right now, there aren't any missing values. However, we have string values that could be understood as missing.\n",
    "* Several features must be converted to numeric.\n",
    "    * There are several binary features, including the target variable, as well as categorical values, each of them indicating a different category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAUoBmbMvfwt"
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "When analyzing `TotalCharges`, we have identified that the missing data was entered as the string `\"\"`. If we simply try to transform it into float(\"\"), the interpreter returns an error.\n",
    "\n",
    "So, in order to convert it properly, I created a function to handle this exception. In the end, I identified missing data and replaced it with the median value of the column.\n",
    "\n",
    "Below, I also took the opportunity to delete the customerID column, which would be unnecessary throughout this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHYzCkfztHv2",
    "outputId": "5def0a6f-0362-40e9-9eeb-f859c6e21a9d"
   },
   "outputs": [],
   "source": [
    "def converter_str_float(entry):\n",
    "    \"\"\"\n",
    "    Converting string objects into float \n",
    "\n",
    "    # Arguments\n",
    "        entry: string\n",
    "\n",
    "    # Returns\n",
    "        If possible, value converted as float.\n",
    "        NaN, if the transformation isn't possible.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return float(entry)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# copying the dataframe\n",
    "df_clean = df.copy()\n",
    "\n",
    "# removing custumerID\n",
    "df_clean.drop(\"customerID\", axis=1, inplace=True)\n",
    "\n",
    "# converting strings in TotalCharges to float\n",
    "df_clean[\"TotalCharges\"] = df_clean[\"TotalCharges\"].apply(converter_str_float)\n",
    "\n",
    "# checking for missing data\n",
    "print(\"Missing Data in 'TotalCharges' (before replacing with Median):\\t{}\\n\".format(df_clean[\"TotalCharges\"].isnull().sum()))\n",
    "\n",
    "# replacing NaN with Median\n",
    "TotalChargesMedian = df_clean.TotalCharges.median()\n",
    "df_clean[\"TotalCharges\"].fillna(TotalChargesMedian, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjYyOj08Swo8"
   },
   "source": [
    "Even though the dataset is mostly composed of binary or categorical variables, it is possible to see below, using the `describe` method, that there are no indications of the presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "e7bMAWBLGL89",
    "outputId": "2755fad2-46a8-4d7f-f9e7-f7806f5364b5"
   },
   "outputs": [],
   "source": [
    "df_clean.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6TedGS1UWUZ"
   },
   "source": [
    "Analyzing boxplots for `MonthlyCharges` and `TotalCharges`, the distribution seems to reinforce our initial hypothesis. \n",
    "\n",
    "There doesn't seem to be data points above or below the limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "vyg2LHeWTHc-",
    "outputId": "e78a1f71-395a-4b52-8204-5b95180a49d4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 7))\n",
    "sns.boxplot(df_clean['MonthlyCharges'], ax=ax[0])\n",
    "sns.boxplot(df_clean['TotalCharges'], ax=ax[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylgfUKN5UzBU"
   },
   "source": [
    "In this case, we can take the variable `tenure`, which indicates how long the client has been a client, as an indication of the quality of the relationship of the client with the company.\n",
    "\n",
    "Since we don't have an official documentation regarding the dataset, let's assume the time unit used in this variable is months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "WH-wPhe6TnqY",
    "outputId": "dc0c8e50-3cb9-418a-93d1-c33f3b4c095d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.countplot(df['tenure'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oftqiyPTVIrD"
   },
   "source": [
    "When dealing with categorical variables, it's important to check for unique values. It allows us to identify when different labels can mean the same thing.\n",
    "\n",
    "For example, you can see below that we have the following entries:`No`, `No phone service` and `No internet service`, which can mean the same thing.\n",
    "\n",
    "It's a valid point. However, since we can't know for sure, let's consider them as different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRWb2qEvsD1D",
    "outputId": "050322c4-33f8-45b2-a6df-e51d6015cde3"
   },
   "outputs": [],
   "source": [
    "# checking unique values for object variables\n",
    "np.unique(df_clean.select_dtypes('object').values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5NpQBCTx4Xp"
   },
   "source": [
    "To identify if the dataset shows any kind of bias, let's check the count of `Male` and `Female`.\n",
    "\n",
    "This type of bias doesn't seem relevant in this specific case. However, there are many instances where it could be a real problem. For instance, if we were working on a project related to a hiring process.\n",
    "\n",
    "As we can see below, the distribution is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "ESzwVJTqzi2P",
    "outputId": "2eb190f2-188d-4730-a598-40b8332ccabb"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "Xx76BqGOIj8Q",
    "outputId": "86e7104c-0a3e-456b-e043-d8ec81e11a52"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='gender', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuqsgoTRWN-e"
   },
   "source": [
    "Last but not least, it's important to check whether the dataset is balanced or not, when it comes to the target variable.\n",
    "\n",
    "As we can see below, `No` is a lot more common than `Yes`. This must be taken into account when building a **Machine Learning** model for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "CVGx_XPfIq5A",
    "outputId": "f7c58f9d-748d-4fef-c574-29e6c5659d88"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Churn', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DXfEV2oyIGI"
   },
   "source": [
    "## Data Pre-Processing for Machine Learning\n",
    "\n",
    "Let's prepare the data in order to fit into a Machine Learning model.\n",
    "\n",
    "### The Process\n",
    "\n",
    "For binary features, I will use `LabelEncoder`. This includes our target variable, `Churn`.\n",
    "\n",
    "The next step is separating numerical and categorical variables. Categorical variables will be transformed into Dummy Variables, so they can be used by different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "GoyYiYQtyKuX",
    "outputId": "a0f92113-50a1-45f8-d9b2-171581ebc58f"
   },
   "outputs": [],
   "source": [
    "# pre-processing variables\n",
    "binary_var = df_clean.nunique()[df_clean.nunique() == 2].keys().tolist()\n",
    "num_var = [col for col in df_clean.select_dtypes(['int', 'float']).columns.tolist() if col not in binary_var]\n",
    "cat_var = [col for col in df_clean.columns.tolist() if col not in binary_var + num_var]\n",
    "\n",
    "# creating a copy of the dataframe\n",
    "df_proc = df_clean.copy()\n",
    "\n",
    "# Label Encoding for Binary Variables\n",
    "le = LabelEncoder()\n",
    "for i in binary_var:\n",
    "    df_proc[i] = le.fit_transform(df_proc[i])\n",
    "\n",
    "# Encoding for multi-class variables\n",
    "df_proc = pd.get_dummies(df_proc, columns=cat_var)\n",
    "\n",
    "# checking the new dataframe\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nHC4RsngDBb"
   },
   "source": [
    "## Machine Learning\n",
    "\n",
    "With the previous pre-processing, the data is ready to be used in a Machine Learning model.\n",
    "\n",
    "Before we go into hyperparameter tuning, feature engineering, feature selection, etc, let's fit the data into a simple model, to be used as baseline for the project.\n",
    "\n",
    "However, it's important to split the data here, before we move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCM1TOKd2Cg7"
   },
   "outputs": [],
   "source": [
    "# isolating the target variable\n",
    "X = df_proc.drop('Churn', axis=1)\n",
    "y = df_proc['Churn']\n",
    "\n",
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2842RiPyYgRe"
   },
   "source": [
    "I will use `cross-validation` to estimate the error for our baseline.\n",
    "\n",
    "To make the process easier, I created a `val_model` function.\n",
    "\n",
    "The models will be evaluated on `recall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46Qxn4ohHEAF"
   },
   "outputs": [],
   "source": [
    "def val_model(X, y, clf, quite=False):\n",
    "    \"\"\"\n",
    "    Cross Validation Function.\n",
    "\n",
    "    # Arguments\n",
    "        X: DataFrame, independent variables.\n",
    "        y: Series, vector, target variable.\n",
    "        clf: Scikit-learn Classifier.\n",
    "        quiet: bool, indicating whether the function should print the results.\n",
    "\n",
    "    # Returns\n",
    "        float, cross-validation avergae score.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    pipeline = make_pipeline(StandardScaler(), clf)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='recall')\n",
    "\n",
    "    if quite == False:\n",
    "        print(\"Recall: {:.2f} (+/- {:.2f})\".format(scores.mean(), scores.std()))\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io8qe_UDZxYg"
   },
   "source": [
    "For the baseline, I will use `Random Forest`, without any type of tuning, with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lS33QG83Iq2r",
    "outputId": "af7733ae-0125-46e4-be7f-b561b0d1dea7"
   },
   "outputs": [],
   "source": [
    "# creating baseline\n",
    "rf = RandomForestClassifier()\n",
    "score_baseline = val_model(X_train, y_train, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3O6FyePWh-fY"
   },
   "source": [
    "Now that we have a baseline, let's move on, and try to improve our results.\n",
    "\n",
    "For this project, after some testing I've found that `Under Sampling` yielded the best results. Also, following recommendations for Under Sampling, I've decided to apply `Standard Scaler` to the data.\n",
    "\n",
    "**Warning**\n",
    "\n",
    "Keep in mind that only the **training** data will be balanced. The test data must remain untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGN48WLmy1gf"
   },
   "outputs": [],
   "source": [
    "# standardizing the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "# oversample = SMOTE()\n",
    "# X_train, y_train = oversample.fit_sample(X_train, y_train)\n",
    "\n",
    "# under sampling\n",
    "rus = RandomUnderSampler()\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xvx5Fsgbvq9"
   },
   "source": [
    "Now, let's try fitting diferrent models for cross validation, to find out which model works best for our specific problem.\n",
    "\n",
    "* Random Forest\n",
    "* Decision Tree\n",
    "* Stochastic Gradient Descent\n",
    "* SVC\n",
    "* Logistic Regression\n",
    "* LightGBM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "hPxSpzXR1u7I",
    "outputId": "07eff2fc-7124-434f-c2ec-1b7c2cdf37e2"
   },
   "outputs": [],
   "source": [
    "# instantiating the models\n",
    "rf = RandomForestClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "sgdc = SGDClassifier()\n",
    "svc = SVC()\n",
    "lr = LogisticRegression()\n",
    "xgb = XGBClassifier()\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "model = []\n",
    "recall = []\n",
    "# evaluating results (recall)\n",
    "for clf in (rf, dt, sgdc, svc, lr, xgb, lgbm):\n",
    "    model.append(clf.__class__.__name__)\n",
    "    recall.append(val_model(X_train_rus, y_train_rus, clf, quite=True))\n",
    "\n",
    "pd.DataFrame(data=recall, index=model, columns=['Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy_HKjoAdHHi"
   },
   "source": [
    "Given the characteristics of the problem and the performance during previous executions, I opted for XGBoost, which was practically tied with Logistic Regression.\n",
    "\n",
    "My understanding is that parameter tuning can further improve the quality of the solution. After some testing adjusting parameters for Logistic Regression and some of the other top performing models, XGBoost performed better after optimization.\n",
    "\n",
    "## Hyperparameter Optimization\n",
    "\n",
    "XGBoost has numerous parameters. Some have a greater impact on the model's quality, while others have less. A good practice is to set a learning rate and number of estimators, tune other parameters, and then check other learning rates.\n",
    "\n",
    "Below, I will adjust the number of estimators. The model was instantiated with `learning_rate=0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQw3kNLVjUNV",
    "outputId": "b72ae3a2-4d45-4d68-b249-bfee15164445"
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.1)\n",
    "\n",
    "param_grid = {\n",
    " 'n_estimators':range(0,1000,50),\n",
    "}\n",
    "\n",
    "# identifying the best parameters\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring=\"recall\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# checking results\n",
    "print(\"Best: {} for {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SV5128AZwAtu"
   },
   "source": [
    "With the number of estimators set at 50, let's optimize `max_depth` and `min_child_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_4EDsdBg8p7",
    "outputId": "be030266-8ca1-445b-d573-7c36664e3cd6"
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.1, n_estimators=50)\n",
    "\n",
    "param_grid = {\n",
    " 'max_depth':range(1,8,1),\n",
    " 'min_child_weight':range(1,5,1)\n",
    "}\n",
    "\n",
    "# identifying the best parameters\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring=\"recall\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# checking results\n",
    "print(\"Best: {} for {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn_gKACfwLBa"
   },
   "source": [
    "After optimization, we have found that the ideal parameters are `max_depth=1` and `min_child_weight=1`.\n",
    "\n",
    "Next step, let''s optimize `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3mzVW_pjEQw",
    "outputId": "41ea0b6f-778f-48b0-ff9f-417f0b345c59"
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.1, n_estimators=50, max_depth=1, min_child_weight=1)\n",
    "\n",
    "param_grid = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "# identifying the best parameters\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring=\"recall\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# checking results\n",
    "print(\"Best: {} for {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAjrnu1rwVRy"
   },
   "source": [
    "Finally, with `gamma=0`, let's test other values for `learning_rate` to make sure we have the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KssnT-wYoxXO",
    "outputId": "f99369cd-4958-4408-dd80-20cfba9d2aa3"
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators=50, max_depth=1, min_child_weight=1, gamma=0.0)\n",
    "\n",
    "param_grid = {\n",
    " 'learning_rate':[0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# identifying the best parameters\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring=\"recall\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# checking results\n",
    "print(\"Best: {} for {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oS2jW6nbwfVg"
   },
   "source": [
    "Assuming `recall` is the most important metric for this projetct, we can see that a lower value for `learning_rate` was the best call.\n",
    "\n",
    "Now, it's time to train the model, and check the results on the test dataset. This is going to be the first time the model receives the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "Zt0_dbd68VxU",
    "outputId": "233169ac-4298-417e-e32d-2b133bbcc42d"
   },
   "outputs": [],
   "source": [
    "# final model\n",
    "xgb = XGBClassifier(learning_rate=0.001 , n_estimators=50, max_depth=1, min_child_weight=1, gamma=0.0)\n",
    "xgb.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# making predictions\n",
    "X_test = scaler.transform(X_test)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# plotting area under the curve\n",
    "print(\"AUC: {:.4f}\\\\n\".format(roc_auc_score(y_test, y_pred)))\n",
    "\n",
    "# 1. Plot Confusion Matrix\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 7)) # Create a figure with two subplots\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true', ax=ax[0])\n",
    "ax[0].set_title(\"Normalized Confusion Matrix\")\n",
    "\n",
    "RocCurveDisplay.from_estimator(xgb, X_test, y_test, ax=ax[1])\n",
    "ax[1].set_title(\"ROC Curve\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTqUkT4aw3vW"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "The results were satisfactory, taking into account that `recall` was the main metric for optimization. Test and training results are close, which is a good sign, and indicates the parameters have been decently tuned.\n",
    "\n",
    "There's still a lot of room for imporivement. We can try different balancing techniques, feature engineering, feature selection, etc.\n",
    "\n",
    "This is a very complex and interesting project, where the data analysis and machine learning can be really helpful.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
